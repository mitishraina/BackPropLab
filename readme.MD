# BackPropLab
BackPropLab or Back Propogating to Classics is a personal code sprint where I implement every algorithm I've studied across Machine Learning, Deep Learning(Neural Networks), NLP, Transformers(Tokenizer, FFN, Normalization etc.)
All implementations are built from scratch, with or without libraries, focusing on mathematical correctness, gradient flow, and system level understanding.

This repo serves two purposes:
1. A structured knowledge base for revisiting core algorithms.
2. A deep dive laboratory for understanding learning dynamics beyond black box frameworks.

**BackPropLab** is my way of builiding intuition, layer by layer, derivative by derivative. It will help to rebuild strong first prinicples intuition, strenthen mathematical rigor, build a long term reference library.

## What's being implemented?
- Classical Machine Learning(Regression, Classification, SVM, KNN etc)
- Deep Learning(Neural Networks, Backpropogation, Activation Functions, CNNs, RNNs, LSTMs etc)
- NLP Foundations(TF-IDF, Attention Mechanism, Seq2Seq models etc.)
- Transformers & LLMs(Scaled Dot Product, Multi Head, Encoder-Decoder, PEFT etc.)


### NOTE: This is an active implementation sprint.
### WOL = Without Library, WL = With Library